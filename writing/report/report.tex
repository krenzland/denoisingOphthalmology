\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[american]{babel}
\usepackage[autostyle, english = american]{csquotes}
 \usepackage[%
   backend=biber]{biblatex}
 \addbibresource{../bibliography.bib}
\usepackage[final]{microtype}
\usepackage{todonotes}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{mathtools} % for \mathclap
\usepackage{hyperref}
\usepackage[noabbrev]{cleveref}
\newcommand{\creflastconjunction}{, and\nobreakspace} % use Oxford comma
%\usepackage{placeins} % for floatbarrier

\usepackage{tikz}
\usetikzlibrary{arrows, positioning, shapes.geometric}
\usetikzlibrary{calc}

\begin{document}
\title{Deep-Learning-based Image Denoising in Ophthalmology}
\author{Lukas Krenz}

\maketitle

\section{Introduction}
\todo{Write introduction}
...

\section{Literature Review}

\subsection{Architectures}
\label{sec:architecturs}
Differentiate between methods optimised for quality and speed.
Quality mostly feeds in a upsampled image, filters for in HR-space.
Efficiency uses low-resolution image as input, upscales at end.

We are mostly interested in networks that use a structure that is optimized for efficiency.
All these structures (or at least the cited examples) have one thing in common:
They use the low-resolution image as input.
\begin{description}
\item[Laplacian Pyramid] Structure used by~\cite{LapSRN}.
  Progressive upsampling, predict subband residual for each upsampling step.
  Feature extraction at \textit{coarse}, not \textit{fine} level.
  Same network can do upsampling at different scales.
  Uses deep supervision, each resulting image contributes to the loss.
\item[Hourglass] Used for example by~\cite{Fsrcnn}.
  Structure of form (Feature extraction -> Shrinking -> Mapping -> Expanding -> Deconvolution).
  The filters (except deconvolution) are similar for different upscaling factors, makes retraining easy.
\item[Subpixel Convolution]  Used by~\cite{Espcn}.
  $L$ convolution layers for feature maps, one subpixel convolution layer with $L$ filters.
  In~\cite{Espcn} $L = 3$.
\end{description}

The following architectures/papers are optimised for quality without regarding the run-time:
\begin{description}
\item[Residual Blocks] Used for example by~\cite{EnhanceNet, SRGAN, EDSR}.
   In case of \textbf{EnhanceNet}: structure of form (Conv -> Residual* -> (NNUpsampling -> Conv)* -> Conv) with 10 residual blocks.
  
\item[Deeply Recursive] Used by~\cite{DRCN}.
  Extremely expensive to train (6 days on Titan X).
  Training relies on several tricks.
  Structure of (embedding network -> recursive inference network -> reconstruction network)
\end{description}

More architectures are discussed in the paper~\cite{NTIRE2017} which describes a contest that was won by~\cite{EDSR}.
Most approaches used in this challenge are not real-time capable.

\subsection{Loss functions}
\label{sec:loss}

\begin{description}
\item[MSE] Standard pixelwise mse, optimizes PSNR, leads to blurry images
\item[Charbonnier-Loss] Pixelwise (differentiable approximation of) $L_1$ error, is more robust, used by~\cite{LapSRN}.
  Defined typically as
  \begin{align}
    \label{eq:charbonnier}
    L( \hat{\bm{y}}, \bm{y}; \bm{\theta}) = \sum_n p \left( \bm{\hat{y}} - \bm{y} \right)
    \intertext{with }
    p(\bm{x}) = \sqrt{ \langle x, x \rangle  + \epsilon^2},
  \end{align}
  where $\varepsilon$ is a parameter.
\item[Perceptual Loss] Don't compare pixel-wise but consider difference of filtered images.
  Can be defined for an arbitrary filter map as
  \begin{equation}
    \label{eq:perceptual-loss}
   L_p = \Vert \phi( \bm{\hat{y}} ) - \phi (\bm{y}) \Vert_2^2 .
  \end{equation}
  The filter maps $\phi$ are often taken from the pooling layers of the VGG-19 network, e.g. EnhanceNet uses the second and fifth~\cite{EnhanceNet}.
  Leads to visually better images but decreased PSNR.
  See~\cite{PerceptualLoss} for motivation/details.
\item[Saliency Loss] The same as the perceptual loss but with handcrafted filters.
  One example for this is~\cite{SaliencyGAN} which uses the curvature of the image, distribution of elements in a neighborhood and uniqueness of feature maps.
  This loss is handcrafted for vessel-detection.
\item[Texture Loss] Use patch-wise correlations of perceptive loss, similar to style transfer. See~\cite{EnhanceNet}.
\item[Adversarial Loss] Optimize linear combination of other loss and loss function learned by discriminator/critic network.
  Leads to visually pleasing images, reduced PSNR and might lead to ``imagined'' artifacts.
  First seen in~\cite{SRGAN}.
  Mostly classical GAN architectures are used, might make sense to explore more recent variants for easier training (e.g.\ WGAN-GP).
  
\end{description}
\subsection{Evaluation}\label{sec:evaluation}
\begin{description}
\item[PSNR] Peak-Signal-To-Noise-Ratio. Scaled version of \textsc{mse}, higher values correspond to lower error, defined by:
  \begin{equation}
  \label{eq:psnr}
  \mathrm{PSNR} = -10 \log_{10} (\mathrm{MSE}).
\end{equation}
\item[SSIM] Structural similarity index. Tries to measure perceived quality of images.
\end{description}

Either use metrics such as PSNR/SSIM, compare visually, or use efficiency as pre-processing, e.g. improved segmentation results

\subsection{Relevant results}
Difficult to compare, visual quality does not correspond directly to improved error!
For \textsc{psnr}-results see~\cref{tab:results}.

\begin{table}[]
\centering
\caption{Some Results, all measured in \textsc{psnr}}

\label{tab:results}
\begin{tabular}{@{}llll@{}}
\toprule
Model          & SET5  & SET14 & BSDS100 \\ \midrule
EDSR+~\cite{EDSR} & 32.62 & 28.94 & 27.97 \\
EnhanceNet-E~\cite{EnhanceNet}   & 31.74 & 28.42 & 27.50   \\
LAPSRN~\cite{LapSRN}         & 31.53 & 28.19 & 27.32   \\
DRCN~\cite{DRCN}           & 31.53 & 28.04 & 27.23   \\
ESPCN~\cite{Espcn}       & 30.9  & 27.73 &         \\
FSRCNN~\cite{Fsrcnn}         & 30.71 & 27.7  & 26.97   \\
EnhanceNet-PAT~\cite{EnhanceNet} & 28.56 & 25.77 & 24.93   \\
Bicubic        & 28.42 & 26.1  & 25.96   \\ \bottomrule
\end{tabular}
\end{table}

\section{Architecture}
We use an architecture that is similar to \textsc{LapSrn} presented in~\cite{LapSRN}.
That means that we use a network consisting of two branches.
The first branch is called \textbf{image reconstruction branch} and upscales the low resolution image using simple upscaling filters.
The second branch extracts features from the low resolution image and predicts the \textbf{residual} for each upscaled image of the first branch.

Our model differs in two ways from the original architecture:
\begin{itemize}
\item We use three-channel \textsc{rgb} images both as input and output instead of working only on the Y-channel.
\item Instead of transposed convolutions we use resize-convolution blocks.
  They are composed of a nearest neighbour interpolation and a standard convolution filter.
  Empirical evidence suggest that they lead to fewer interpolation artefacts than transposed convolution, especially when they are used in a \textsc{gan} framework\cite{deconvolution}.
\end{itemize}

We use the PatchGan discriminator presented in~\cite{PatchGAN}.
This is a fully convolutional discriminator that penalizes on a basis $70\times70$ patches.
In our case this can be interpreted as a very coarse texture loss.

\section{Loss functions}
We use a combination of a saliency-weighted L1-loss, an perceptual loss and an adversarial loss for our experiments.

The Charbonnier-Loss is defined by
\begin{align}
\label{eq:charbonnier}
    L( \hat{\bm{y}}, \bm{y}; \bm{\theta}) = \sum_n p \left( \bm{\hat{y}} - \bm{y} \right),
    \intertext{with }
    p(\bm{x}) = \sqrt{ \langle x, x \rangle  + \epsilon^2},
\end{align}
where $\varepsilon$ is a parameter set to $1e-6$ for our experiments.
This is a differentiable approximation of the $L_1$ loss which leads to sharper edges compared to a standard \textsc{mse}-loss.
Additionally, results for our chosen architecture indicate that choosing this loss leads to a faster convergence~\cite{LapSRN}.

Secondly, we use a perceptual loss.
The idea is to not compare images pixel-wise but consider difference of filtered images.
This loss can be defined for an arbitrary filter map as
\begin{equation}
  \label{eq:perceptual-loss}
  L_p = \Vert \phi( \bm{\hat{y}} ) - \phi (\bm{y}) \Vert_2^2 .
\end{equation}
We use filter maps $\phi$ taken from the pooling layers of a pre-trained VGG-19 network.
This leads to visually better images but decreased PSNR.
See~\cite{PerceptualLoss} for motivation and details.

While the images in standard super-resolution tasks do not share a lot of common structure, retinal fundus images are visually similar to each other.
This can be used to create hand-crafted filters.
We use the saliency maps of~\cite{SaliencyGAN}.
They combine the curvature of the images and the entropy of pixels.

As a final building block we use an adversarial loss.
Training the model now resembles a two-player game between a generator an a discriminator network.
The discriminator tries to decide, whether an image is a ground-truth high-resolution image or an low resolution image that was upsampled by the generator.
This is realized as the optimization problem~\cite{GAN}
\begin{align}
  \cdots.
\end{align}
To improve numerical stability, the discriminator optimizes \(\cdots\) instead.

The first implementation that used an adversarial loss for single-image super-resolution was~\cite{SRGAN} which resulted in visually pleasing images.
\todo{Describe imagined artefacts?}
\todo{Only standard GAN? or also wgan?}

\section{Training}
We use the Messidor dataset~\cite{Messidor} which consists of 1200 high resolution fundus images.
We used 80\% of the dataset for training, the rest was used for validation.
The images were augmented by random horicontally and vertically flips, scaling by a factor between $0.5$ and $1.0$ and rotation by 0, 90, 180 or 270 degrees \todo{check rotation}.
As a last augmentation, a random crop of size 128 is chosen.
We try to sample crops that contain more information, to do this we reject crops which either contain too many black pixels or too few blood vessel pixels.
The blood vessels are detected using a Frangi-filter \todo{cite frangi filter}.
We use a batch size of $64$ for all experiments.

Both networks are optimized by \textsc{adam}~\cite{adam} with an initial learning rate of $1r-4$.
The generator is trained for $10^5$ gradient iterations, after this the learning rate is decreased to $1e-5$ and the network is trained for another $10^5$ iterations.
We first train solely the generator without the adversarial loss.
The resulting network is then used to initialise the adversarial training, which continues for another $10^5$ iterations.
We validate after every $1005$ update iterations.



\section{Evaluation}
...
\todo{Cite SSIM}
\todo{PSNR table}
\todo{Segmentation table, Frangi AND Retina-U-NEt}
\todo{Cite retina unet (link)}
\todo{Cite DRIVE database}
\todo{Intraoperative evaluation}
\section{Summary}
...

\printbibliography
\end{document}